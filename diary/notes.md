---
layout: page
title: Diary
categories: notes
---

#### 16th December, 2024
#####
 * LLM context window is the amount of text an LLM can take as input and formulate a response for. Prompt engineering is tweaking the text in this context window.
 * The longer the input in the context window, the higher the compute cost.
 * Neural attention memory models (NAMMs) can be used to eliminate redundant tokens and improve performance.

#### 27th Octoboer, 2025
##### Big Oh notation
 * Big Oh notation isn't measuring how fast your code runs for any given moment in time. It's trying to estimate how slow your code will run as data grows.
 * For small enough data sizes, it isn't worth thinking too much about algorithmic complexity. Just proceed with using the simplest approach.
